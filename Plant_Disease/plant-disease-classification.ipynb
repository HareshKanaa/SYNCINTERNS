{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":658267,"sourceType":"datasetVersion","datasetId":277323},{"sourceId":6975847,"sourceType":"datasetVersion","datasetId":3923798}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Plant Disease Detection with Plant Village and PlantDoc\n## Import Libraries and configure directories","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import classification_report, roc_auc_score\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras","metadata":{"id":"bYWb1UN6sqmv","execution":{"iopub.status.busy":"2023-12-10T11:27:51.389784Z","iopub.execute_input":"2023-12-10T11:27:51.390293Z","iopub.status.idle":"2023-12-10T11:28:00.178978Z","shell.execute_reply.started":"2023-12-10T11:27:51.390265Z","shell.execute_reply":"2023-12-10T11:28:00.177990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plant_village_dir = \"/kaggle/input/plantvillage-dataset/color\"\nplant_doc_dir = \"/kaggle/input/plantdoc-dataset/train\"\ndataset_dir = \"/kaggle/working/dataset/\"\n\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:28:15.706977Z","iopub.execute_input":"2023-12-10T11:28:15.707713Z","iopub.status.idle":"2023-12-10T11:28:15.712658Z","shell.execute_reply.started":"2023-12-10T11:28:15.707677Z","shell.execute_reply":"2023-12-10T11:28:15.711578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network\n### The Neural Network Architecture\n1. **Rescaling Layer (Normalization)**:\n   - Input Shape: (224, 224, 3)\n   - Function: This layer scales the input pixel values to a range between 0 and 1 by dividing each pixel value by 255. This step is often used to ensure that the neural network's initial weights are in a reasonable range and can help improve training.\n\n2. **Convolutional Layers (Conv2D)**:\n   - Conv2D(32, (3,3), activation='relu'): \n     - This layer uses 32 filters (also called kernels) of size 3x3 to extract features from the input. \n     - Activation: ReLU (Rectified Linear Unit) is used as the activation function, which introduces non-linearity.\n     - These convolutional layers are responsible for learning various low-level and mid-level features in the images.\n\n3. **MaxPooling Layers (MaxPool2D)**:\n   - MaxPool2D((2,2)):\n     - Max pooling reduces the spatial dimensions of the feature maps obtained from the convolutional layers by taking the maximum value from a 2x2 window.\n     - It helps reduce the computational complexity and increases the network's ability to focus on the most important features.\n\n4. **Dropout Layers**:\n   - Dropout(0.2):\n     - This layer randomly \"drops out\" (sets to zero) 20% of the output units during training. It helps prevent overfitting by introducing noise and ensuring that the network doesn't rely too heavily on any one feature.\n\n5. **Flatten Layer**:\n   - Function: This layer takes the output from the previous layers and flattens it into a 1D vector. This prepares the data for the fully connected layers.\n\n6. **Fully Connected Layers (Dense)**:\n   - Dense(128, activation='relu'):\n     - This layer consists of 128 neurons and applies the ReLU activation function. It helps to learn high-level features and patterns in the data.\n   - Dense(64, activation='relu'):\n     - Another dense layer with 64 neurons and ReLU activation.\n   - Dense(38, activation='sigmoid'):\n     - The final dense layer with 38 neurons. It uses the sigmoid activation function, which is typical for multi-label classification problems. Each neuron in this layer represents one of the 38 possible classes. The sigmoid function scales the output between 0 and 1, allowing for multi-label classification.","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([\n    keras.layers.Rescaling(scale=1/255, input_shape=(224, 224, 3)),\n    \n    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D((2, 2)),\n    keras.layers.Dropout(0.2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D((2, 2)),\n    keras.layers.Dropout(0.2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D((2, 2)),\n    keras.layers.Dropout(0.2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D((2, 2)),\n    keras.layers.Dropout(0.2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D((2, 2)),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(38, activation='sigmoid')\n])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:28:20.883281Z","iopub.execute_input":"2023-12-10T11:28:20.884152Z","iopub.status.idle":"2023-12-10T11:28:24.078986Z","shell.execute_reply.started":"2023-12-10T11:28:20.884119Z","shell.execute_reply":"2023-12-10T11:28:24.078047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimizer (optimizer='adam'):\n   - An optimizer is a critical component of the training process in neural networks. It defines the specific algorithm used to adjust the model's parameters (weights and biases) during training in order to minimize the chosen loss function.\n   - 'Adam' stands for Adaptive Moment Estimation, and it is a popular optimization algorithm. It combines the benefits of both the Adagrad and RMSprop optimizers. Adam adjusts the learning rates for each parameter individually and maintains moving averages of the gradients.\n   - The optimizer's role is to update the model's weights in a way that helps it converge to a solution that minimizes the loss function.\n\n### Loss Function (loss='sparse_categorical_crossentropy')\n   - The loss function, also known as the cost function or objective function, measures the difference between the predicted values and the actual target values during training.\n   - 'Sparse Categorical Crossentropy' is a loss function commonly used in classification tasks, particularly when dealing with integer-encoded class labels. It calculates the cross-entropy between the predicted probability distribution (after applying a softmax activation) and the true class labels.\n   - In this case, 'sparse' indicates that the class labels are integer values (e.g., 0, 1, 2) rather than one-hot encoded vectors.\n\n### 3. Metrics (metrics='accuracy'):\n   - Metrics are used to evaluate and monitor the performance of the model during training and testing.\n   - 'Accuracy' is a commonly used metric for classification problems. It measures the proportion of correctly predicted samples to the total number of samples. In a multi-class classification task, accuracy calculates the ratio of correctly classified samples to the total number of samples.","metadata":{}},{"cell_type":"code","source":"initial_learning_rate = 0.001\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=1000, decay_rate=0.9\n)\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\nmodel.compile(\n    optimizer=optimizer, \n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:28:30.879411Z","iopub.execute_input":"2023-12-10T11:28:30.880031Z","iopub.status.idle":"2023-12-10T11:28:30.912430Z","shell.execute_reply.started":"2023-12-10T11:28:30.879997Z","shell.execute_reply":"2023-12-10T11:28:30.911509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Early stopping is a technique used during the training of neural networks to prevent overfitting and improve model generalization. It is based on a simple principle: if the model's performance on a validation dataset does not improve or worsens over a certain number of training epochs, training is halted to prevent the model from fitting the training data too closely. Let's explain the key components and the function of early stopping from first principles:\n\n1. **Monitor ('val_loss')**:\n   - The 'monitor' parameter specifies the metric that the early stopping algorithm should monitor to determine when to stop training. In this case, 'val_loss' is used, which means the validation loss is monitored. The validation loss is a measure of how well the model is performing on a separate validation dataset.\n\n2. **Patience (patience=5)**:\n   - The 'patience' parameter defines the number of epochs with no improvement in the monitored metric to wait before stopping the training process. If, for a specified number of consecutive epochs (in this case, 5), the validation loss doesn't decrease or improve, training is stopped. This is a mechanism to prevent the model from overfitting because further training might lead to worse generalization.\n\n3. **Restore Best Weights (restore_best_weights=True)**:\n   - When 'restore_best_weights' is set to 'True,' the early stopping callback will restore the model's weights to the best-performing state observed during training. This is essential because, during training, the model's weights might change in a way that leads to overfitting, but the best weights represent the model with the highest performance on the validation data. Restoring the best weights ensures that the model retains the best generalization ability.\n\nTo understand the function of early stopping, consider the training process:\n\n- The model's performance on the validation dataset is continuously monitored.\n- If, for the specified number of consecutive epochs (patience), the validation loss does not improve or starts to degrade, the training process is stopped early.\n- By stopping training at this point, the model is prevented from overfitting, which can occur if it continues to fit the training data noise.","metadata":{}},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  # You can use other metrics like 'val_accuracy'\n    patience=5,          # Number of epochs with no improvement to wait\n    restore_best_weights=True  # Restore the best model weights when early stopping\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:28:37.568260Z","iopub.execute_input":"2023-12-10T11:28:37.568607Z","iopub.status.idle":"2023-12-10T11:28:37.573224Z","shell.execute_reply.started":"2023-12-10T11:28:37.568577Z","shell.execute_reply":"2023-12-10T11:28:37.572324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model with PlantVillage Dataset\nIn this data-set, 38 different classes of 54305 plant leafs are available. \nThe classes are,\n1. Apple_scab\n1. Apple_black_rot\n1. Apple_cedar_apple_rust\n1. Apple_healthy\n1. Blueberry_healthy\n1. Cherry_powdery_mildew\n1. Cherry_healthy\n1. Corn_gray_leaf_spot\n1. Corn_common_rust\n1. Corn_northern_leaf_blight\n1. Corn_healthy\n1. Grape_black_rot\n1. Grape_black_measles\n1. Grape_leaf_blight\n1. Grape_healthy\n1. Orange_haunglongbing\n1. Peach_bacterial_spot\n1. Peach_healthy\n1. Pepper_bacterial_spot\n1. Pepper_healthy\n1. Potato_early_blight\n1. Potato_healthy\n1. Potato_late_blight\n1. Raspberry_healthy\n1. Soybean_healthy\n1. Squash_powdery_mildew\n1. Strawberry_healthy\n1. Strawberry_leaf_scorch\n1. Tomato_bacterial_spot\n1. Tomato_early_blight\n1. Tomato_healthy\n1. Tomato_late_blight\n1. Tomato_leaf_mold\n1. Tomato_septoria_leaf_spot\n1. Tomato_spider_mites_two-spotted_spider_mite\n1. Tomato_target_spot\n1. Tomato_mosaic_virus\n1. Tomato_yellow_leaf_curl_virus\n\n### Load the dataset","metadata":{}},{"cell_type":"code","source":"train_ds, test_ds = keras.utils.image_dataset_from_directory(\n    plant_village_dir ,\n    image_size=(224,224),\n    batch_size=32,\n    seed = 123,\n    validation_split=.2,\n    subset='both'\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:28:41.645060Z","iopub.execute_input":"2023-12-10T11:28:41.645441Z","iopub.status.idle":"2023-12-10T11:29:05.691210Z","shell.execute_reply.started":"2023-12-10T11:28:41.645409Z","shell.execute_reply":"2023-12-10T11:29:05.690270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_ds, epochs=10, validation_data=test_ds, callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:35:11.152644Z","iopub.execute_input":"2023-12-10T11:35:11.156890Z","iopub.status.idle":"2023-12-10T11:49:58.744713Z","shell.execute_reply.started":"2023-12-10T11:35:11.156854Z","shell.execute_reply":"2023-12-10T11:49:58.743898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(test_ds)\npredicted_labels = tf.argmax(predictions, axis=1)\ntrue_labels = [label for _, label in test_ds.unbatch()]\nprint(classification_report(true_labels, predicted_labels))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:50:06.690960Z","iopub.execute_input":"2023-12-10T11:50:06.691774Z","iopub.status.idle":"2023-12-10T11:50:22.235289Z","shell.execute_reply.started":"2023-12-10T11:50:06.691717Z","shell.execute_reply":"2023-12-10T11:50:22.234357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:50:27.776967Z","iopub.execute_input":"2023-12-10T11:50:27.777326Z","iopub.status.idle":"2023-12-10T11:50:28.361648Z","shell.execute_reply.started":"2023-12-10T11:50:27.777297Z","shell.execute_reply":"2023-12-10T11:50:28.360768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model with PlantVillage and PlantDoc Dataset","metadata":{}},{"cell_type":"code","source":"shutil.copytree(plant_village_dir, dataset_dir)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:40:25.205025Z","iopub.execute_input":"2023-10-29T16:40:25.205298Z","iopub.status.idle":"2023-10-29T16:41:10.06849Z","shell.execute_reply.started":"2023-10-29T16:40:25.205274Z","shell.execute_reply":"2023-10-29T16:41:10.067541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PlantDoc Dataset\nPlantDoc has 27 classes of Plant Diseases.\n1. Apple_Scab_Leaf\n2. Apple_leaf\n3. Apple_rust_leaf\n4. Bell_pepper_leaf\n5. Bell_pepper_leaf_spot\n6. Blueberry_leaf\n7. Cherry_leaf\n8. Corn_Gray_leaf_spot\n9. Corn_leaf_blight\n10. Corn_rust_leaf\n11. Peach_leaf\n12. Potato_leaf_early_blight\n13. Potato_leaf_late_blight\n14. Raspberry_leaf\n15. Soyabean_leaf\n16. Squash_Powdery_mildew_leaf\n17. Strawberry_leaf\n18. Tomato_Early_blight_leaf\n19. Tomato_Septoria_leaf_spot\n20. Tomato_leaf\n21. Tomato_leaf_bacterial_spot\n22. Tomato_leaf_late_blight\n23. Tomato_leaf_mosaic_virus\n24. Tomato_leaf_yellow_virus\n25. Tomato_mold_leaf\n26. grape_leaf\n27. grape_leaf_black_rot\n\n### Merging PlantVillage with PlantDoc\nI have merge the PlantVillage Dataset with PlantDoc with their respective classes as follows","metadata":{}},{"cell_type":"code","source":"class_mapping = {\n    \"Apple_Scab_Leaf\": \"Apple___Apple_scab\",\n    \"Apple_leaf\": \"Apple___healthy\",\n    \"Apple_rust_leaf\": \"Apple___Cedar_apple_rust\",\n    \"Blueberry_leaf\": \"Blueberry___healthy\",\n    \"Cherry_leaf\": \"Cherry_(including_sour)___healthy\",\n    \"Corn_Gray_leaf_spot\": \"Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\",\n    \"Corn_leaf_blight\": \"Corn_(maize)___Northern_Leaf_Blight\",\n    \"Corn_rust_leaf\": \"Corn_(maize)___Common_rust_\",\n    \"Peach_leaf\": \"Peach___healthy\",\n    \"Potato_leaf_early_blight\": \"Potato___Early_blight\",\n    \"Potato_leaf_late_blight\": \"Potato___Late_blight\",\n    \"Raspberry_leaf\": \"Raspberry___healthy\",\n    \"Soyabean_leaf\": \"Soybean___healthy\",\n    \"Squash_Powdery_mildew_leaf\": \"Squash___Powdery_mildew\",\n    \"Strawberry_leaf\": \"Strawberry___healthy\",\n    \"Tomato_Early_blight_leaf\": \"Tomato___Early_blight\",\n    \"Tomato_Septoria_leaf_spot\": \"Tomato___Septoria_leaf_spot\",\n    \"Tomato_leaf\": \"Tomato___healthy\",\n    \"Tomato_leaf_bacterial_spot\": \"Tomato___Bacterial_spot\",\n    \"Tomato_leaf_late_blight\": \"Tomato___Late_blight\",\n    \"Tomato_leaf_mosaic_virus\": \"Tomato___Tomato_mosaic_virus\",\n    \"Tomato_leaf_yellow_virus\": \"Tomato___Tomato_Yellow_Leaf_Curl_Virus\",\n    \"Tomato_mold_leaf\": \"Tomato___Leaf_Mold\",\n    \"grape_leaf\": \"Grape___healthy\",\n    \"grape_leaf_black_rot\": \"Grape___Black_rot\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:41:10.072143Z","iopub.execute_input":"2023-10-29T16:41:10.072425Z","iopub.status.idle":"2023-10-29T16:41:10.07939Z","shell.execute_reply.started":"2023-10-29T16:41:10.072402Z","shell.execute_reply":"2023-10-29T16:41:10.07848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k, v in class_mapping.items():\n    source_dir = f\"/kaggle/input/plantdoc-dataset/train/{k}\"\n    dest_dir = f\"/kaggle/working/dataset/{v}\"\n    file_list = os.listdir(source_dir)\n    for filename in file_list:\n        source_file = os.path.join(source_dir, filename)\n        destination_file = os.path.join(dest_dir, filename)\n        shutil.copy2(source_file, destination_file)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:41:10.080618Z","iopub.execute_input":"2023-10-29T16:41:10.080982Z","iopub.status.idle":"2023-10-29T16:41:30.834524Z","shell.execute_reply.started":"2023-10-29T16:41:10.08095Z","shell.execute_reply":"2023-10-29T16:41:30.833664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the dataset","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/working/dataset'\ntrain_ds, test_ds = keras.utils.image_dataset_from_directory(\n    path ,\n    image_size=(224,224),\n    batch_size=32 ,\n    seed = 123 ,\n    validation_split=.2,\n    subset='both'\n)\n","metadata":{"id":"o1_5m8oKtfdr","outputId":"b9ac1b16-8b9e-4abf-c51a-236c7ea66ed0","execution":{"iopub.status.busy":"2023-10-29T16:41:30.835693Z","iopub.execute_input":"2023-10-29T16:41:30.835996Z","iopub.status.idle":"2023-10-29T16:41:34.43087Z","shell.execute_reply.started":"2023-10-29T16:41:30.83597Z","shell.execute_reply":"2023-10-29T16:41:34.429879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_ds, epochs=50, validation_data=test_ds, callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:41:34.432641Z","iopub.execute_input":"2023-10-29T16:41:34.433004Z","iopub.status.idle":"2023-10-29T16:52:45.541655Z","shell.execute_reply.started":"2023-10-29T16:41:34.432978Z","shell.execute_reply":"2023-10-29T16:52:45.540647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(test_ds)\npredicted_labels = tf.argmax(predictions, axis=1)\ntrue_labels = [label for _, label in test_ds.unbatch()]\nprint(classification_report(true_labels, predicted_labels))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:52:45.543212Z","iopub.execute_input":"2023-10-29T16:52:45.543514Z","iopub.status.idle":"2023-10-29T16:53:19.55898Z","shell.execute_reply.started":"2023-10-29T16:52:45.543488Z","shell.execute_reply":"2023-10-29T16:53:19.557916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels_one_hot = tf.one_hot(true_labels, depth=38)\npredicted_labels_one_hot = tf.one_hot(predicted_labels, depth=38)\nroc_auc = roc_auc_score(true_labels_one_hot, predicted_labels_one_hot, average='macro')\nprint(\"ROC AUC Score:\", roc_auc)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:53:19.560523Z","iopub.execute_input":"2023-10-29T16:53:19.56096Z","iopub.status.idle":"2023-10-29T16:53:21.796267Z","shell.execute_reply.started":"2023-10-29T16:53:19.560911Z","shell.execute_reply":"2023-10-29T16:53:21.795345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:53:21.797635Z","iopub.execute_input":"2023-10-29T16:53:21.797934Z","iopub.status.idle":"2023-10-29T16:53:22.187424Z","shell.execute_reply.started":"2023-10-29T16:53:21.79791Z","shell.execute_reply":"2023-10-29T16:53:22.186569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance Comparison\nThis values from first version of the notebook. \n\n|   |Precision| Recall |F1 Score|Accuracy|No of Epoches|No of Samples|\n|---|---|---|---|---|---|---|\n| PlantVillage  |0.71|0.62|0.61|0.62|6|10861|\n| PlantDoc train + Plant Village |0.86|0.85|0.85|0.85|18|11306|\n","metadata":{}},{"cell_type":"markdown","source":"## Further Enhancements Can be Done\n### 1. Data Augmentation:\n   Data augmentation is a technique in the field of machine learning and computer vision that can significantly enhance the quality and diversity of your dataset. It involves applying various transformations and modifications to your existing data, such as flipping, rotating, cropping, and introducing noise. This process generates additional training samples, reducing the risk of overfitting and improving the generalization capability of your model. Data augmentation can be particularly valuable in scenarios with limited labeled data, helping to create a more robust and comprehensive training set. By leveraging data augmentation, you can make your machine learning models more resilient and capable of handling variations and noise in real-world data.\n\n### 2. Using Transfer Learning:\n   Transfer learning is a powerful approach in deep learning where a pre-trained model, typically trained on a large dataset, is fine-tuned or used as a feature extractor for a different but related task. This technique offers several advantages, including faster convergence, better performance with limited data, and the ability to leverage the knowledge and features learned by the pre-trained model. By adopting transfer learning, you can save both time and computational resources in training deep neural networks from scratch. It is especially valuable when working with smaller datasets or when developing models for specific applications, as it allows you to harness the wisdom of models trained on extensive and diverse data sources. Overall, transfer learning is a practical strategy for improving the efficiency and effectiveness of deep learning models in various applications.","metadata":{}}]}